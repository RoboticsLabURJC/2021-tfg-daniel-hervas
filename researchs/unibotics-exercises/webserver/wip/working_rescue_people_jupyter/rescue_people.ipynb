{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescue People Practice\n",
    "---\n",
    "<img src=\"resources/jderobot.png\" width=\"15%\" height=\"15%\" style=\"float:left;padding-right:5px\"/>\n",
    "\n",
    "## 1 - Introduction\n",
    "---\n",
    "The intention of this practice is to implement the logic that allows a quadricopter to reognize recognize lost people (their faces) and save their position on a given map, in order to perform a subsequent rescue maneuver.\n",
    "\n",
    "For this practice a world has been designed for the Gazebo simulator (see section 2.1). The main task will consist of implementing a face detection and processing that information.\n",
    "\n",
    "To do so, the student needs to have at least the next knowledge:\n",
    "* Basic knowledge of the architecture and operation of drones\n",
    "* Python programming skills\n",
    "* Color spaces (RGB, HSV, etc)\n",
    "* Basic understanding of [OpenCV library](http://opencv.org/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 - Exercise components\n",
    "---\n",
    "<img src=\"resources/rescuepeople.png\" width=\"50%\" height=\"50%\" style=\"float:right;padding-right:15px;margin-top:35px;\"/>\n",
    "\n",
    "### 2.1 Gazebo Simulator\n",
    "\n",
    "Gazebo simulator will be running in the background. The Gazebo world employed for this exercise has a 3D model of the AR.Drone and 5 simulated people (disposed as shown in the image). The intention is to make the drone create a route through which it is able to cover the entire extension of the map (starting from the initial position marked), in such a way that it can go recognizing the land in search of the lost people contained in it.\n",
    "\n",
    "The AR.Drone quadrotor will provide images from its vertical camera where those people will be visualized.\n",
    "\n",
    "### 2.1 Rescue People Component\n",
    "This component has been developed specifically to carry out this exercise. This component connects to Gazebo to teleoperate the quadrotor (or send orders to it) and receives images from its camera. The student has to modify this component and add code to accomplish the exercise. In particular, it is required to modify the execute() method.\n",
    "\n",
    "### 2.3 Printer\n",
    "This class prints an image in a Jupyter Notebook for debugging purposes. It will receive processed images from Rescue People to debug our algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Exercise initialization\n",
    "---\n",
    "First of all, we need to connect with the drone through Gazebo simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "simulator = subprocess.Popen((\"gazebo\", \"ArDrone_rescue-people.world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to code, we need to call ``Rescue People`` class once. Run this code and wait a few seconds until rescue people initialization finishes with an ``OK`` message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from rescue_people import RescuePeople\n",
    "from printer import printImage\n",
    "from datetime import datetime\n",
    "\n",
    "rp = RescuePeople()\n",
    "rp.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to send the takeoff order to the drone, so you can see anything on its camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.extra.takeoff()  #We will go over the drone's API in step 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start coding to give intelligence to the drone. We can do it modifying the execute() method from Rescue People component. This method will be called iteratively about 10 times per second. To understand how it works, we are going to print a message in each iteration. For that, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the execute method\n",
    "def execute(self):\n",
    "    print \"Running execute iteration\"\n",
    "      \n",
    "rp.setExecute(execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop printing the updating of the method with an empty instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(self):\n",
    "    pass\n",
    "    \n",
    "rp.setExecute(execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time for you to begin to approach the exercise. Remember to subdivide the different problems to be addressed in small, manageable tasks to make it easier for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - API:\n",
    "---\n",
    " - Te way to send orders to the robot is the following:\n",
    "```\n",
    "self.cmdvel.sendCMDVel(valueX,valueY,0,0,0,0)\n",
    "```\n",
    "\n",
    "- You can also get navigation data with:\n",
    "```\n",
    "data = self.navdata.getNavData() \n",
    "```\n",
    "This will give you state (data.state), altitude (data.altd), vehicle (data.vehicle) and battery (data.batteryPercent).\n",
    "\n",
    "\n",
    "- Get the image from the camera as:\n",
    "```\n",
    "Ã¬mg = self.getImage()\n",
    "```\n",
    "- We provide a cascade classifier object for the segmentation:\n",
    "```\n",
    "self.faceCascade\n",
    "```\n",
    "- Finally, you can use the drone's API, that allows you to:\n",
    "```\n",
    "self.pose.getPose3d().x, self.pose.getPose3d().y, self.pose.getPose3d().z\n",
    "```\n",
    "returns the position values of the drone in space.\n",
    "```\n",
    "self.pose.getPose3d().roll, self.pose.getPose3d().pitch, self.pose.getPose3d().yaw\n",
    "```\n",
    "returns the rotation values of the drone in space.\n",
    "```\n",
    "self.extra.toggleCam()\n",
    "```\n",
    "changes the drone's active camera (frontal or the one below).\n",
    "```\n",
    "self.extra.takeOff()\n",
    "```\n",
    "Takeoff of the drone.\n",
    "```\n",
    "self.extra.land()\n",
    "```\n",
    "landing of the drone.\n",
    "\n",
    "- Finally, you can see the last saved image in execute() method running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageCamera = rp.get_color_image()\n",
    "printImage(imageCamera)\n",
    "\n",
    "#or\n",
    "\n",
    "\n",
    "segmentedImage = rp.get_threshold_image()\n",
    "printImage(segmentedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, you must have saved an image in execute() method. With that purpose, firstly get an image and then use:\n",
    "\n",
    " - to save the last segmented image:\n",
    " ```\n",
    " self.set_threshold_image(img)\n",
    " ```\n",
    " - to save the last camera's image:\n",
    " ```\n",
    " self.set_color_image(img)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividing the practice in different tasks.\n",
    "---\n",
    "### 4.1 - Programming a face segmentation\n",
    "To accomplish this exercise the student has to implement the logic for a <font color=green>face detection</font>, that is, to segmnet any faces on the image and detects its position inside it, and then locate it on the map.\n",
    "\n",
    "Thus, given an input image like this image:\n",
    "\n",
    "![Input image](resources/normal.png \"Input image\")\n",
    "\n",
    "The expected output would be similar to this image:\n",
    "\n",
    "![Output image](resources/segmented.png \"Output image\")\n",
    "\n",
    "This link can be useful to you:\n",
    "- [Face Detection](https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html)\n",
    "\n",
    "### 4.2 - Programming the route\n",
    "\n",
    "Remember that the route that the drone must follow needs to cover all the points of the map, so that details are not lost. Bearing in mind that the predisposed map has a pentagon shape, we suggest using a spiral route, following the edges of the geometric figure and reducing the radius of the spin in each iteration. To make this task easier, you have a variable `` self.AREA`` that contains the coordinates of the vertices of the figure in the world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Algorithm skeleton\n",
    "---\n",
    "We provide a tentative skeleton where you can code your Rescue People. This os one pipeline of the many possible solutions for this practice. Remember that you can follow the one that is easiest for you. We propose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(self):\n",
    "      \n",
    "    #If you need it, initialize your variables \n",
    "    if not self.has_position: \n",
    "        # initialization\n",
    "        \n",
    "        # Calculate your first target\n",
    "        \n",
    "        #save the initial position, so that you will be able to go back to it.\n",
    "        \n",
    "\n",
    "    # TIME TO SEARCH PEOPLE\n",
    "    # Check if we have reached the next target\n",
    "    # If reached:\n",
    "        # Get next position to reach (target)\n",
    "        \n",
    "    \n",
    "    # Read image\n",
    "    img = self.getImage()\n",
    "    #Convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "      \n",
    "    # if faces\n",
    "        # save face\n",
    "        # Get position in 3D\n",
    "        # Change to world coordinates\n",
    "        \n",
    "\n",
    "    # Continue moving to target\n",
    "      \n",
    "\n",
    "    # if search done\n",
    "        # Go back to initial position and print faces' position\n",
    "    \n",
    "    # send orders to the robot\n",
    "    self.cmdvel.sendCMDVel(velx,vely,0,0,0,0)\n",
    "        \n",
    "\n",
    "rp.setExecute(execute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
